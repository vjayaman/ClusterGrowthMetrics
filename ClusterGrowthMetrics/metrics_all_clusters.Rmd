---
title: "metrics_all_clusters"
output: html_document
---

```{r setup, include=FALSE}
stopwatch <- rep(0,2)
stopwatch[1] <- Sys.time()

source("global.R")
source("tabulating_functions.R")

knitr::opts_chunk$set(echo = TRUE)
```

## Synthetic dataset development: 

```{r}
tp2 <- read.csv("3692_2020-04-15_tree-clusters-dm/3692_2020-04-15_thresholds.csv", 
                stringsAsFactors = FALSE, numerals = "no.loss") %>% as_tibble()
  
melted_tp2 <- melt(tp2, id = "isolate") %>% as_tibble()
melted_tp2$h <- melted_tp2$variable %>% as.character() %>% gsub("h_", "", .) %>% as.integer()
colnames(melted_tp2) <- c("isolate", "height", "cluster", "num_h")

heights <- melted_tp2$num_h %>% unique()
  
# clusters and sizes for all heights for TP2
melted_tp2$height %<>% as.character()
cs_sizes <- lapply(colnames(tp2)[-1], function(h) {
  tp2[,h] %>% table() %>% as.data.frame() %>% set_colnames(c("cluster", "size")) %>% 
    as_tibble() %>% bind_cols(height = h, .) %>% return()
}) %>% bind_rows() %>% as_tibble()

cs_sizes$cluster %<>% as.integer()
# | isolate | character height | cluster | number height | cluster size |
  
# Looking at first ~300 heights to identify a useful height for the dropped genome analysis.
# Plotted along these heights to find a noticeable rise in cluster size before a plateau.
  
hvals <- colnames(tp2)[2:301]
isolate_select <- melted_tp2 %>% filter(height %in% hvals)
cs_select <- cs_sizes %>% filter(height %in% hvals)
```

### Plotting cluster numbers and sizes
Zoom in along the x-axis to see in more detail where the plateaus are.

```{r}
# plot_data <- left_join(isolate_select, cs_select, by = c("height", "cluster"))
# plot_data$height <- factor(plot_data$height, levels = unique(plot_data$height))
# # plot_data %<>% filter(num_h %in% 150:190)
# 
# ggplot(plot_data, aes(x = num_h, y = size, group = height)) + geom_boxplot() +
#   geom_vline(xintercept = 177, linetype = "dashed") + ylab("Cluster sizes") +
#   theme(axis.text.x = element_text(angle = 45)) + xlab("Heights") +
#   ggtitle("Close up of section with height used in synthetic TP1 generation")
```

```{r dga}
# Height h_177 was selected, with 493 non-singleton clusters.
# TP2 clusters and sizes
h <- "h_177"
# clusters at TP2 for height 177, for all isolates
t2_height_select <- tp2 %>% dplyr::select(isolate, all_of(h))
sizes <- t2_height_select %>% pull(h) %>% table() %>% as.data.frame() %>% 
  as_tibble() %>% set_colnames(c("cluster", "size"))
sizes$cluster %<>% as.integer()
multi_strain <- sizes %>% filter(size > 1)
t2_cl_over_one <- multi_strain %>% nrow()
  
# Within those multistrain clusters, 60% of those isolates were randomly selected (1188 such isolates). 
# We then filter the TP2 dataset to exclude these isolates, this generates the TP1 dataset. Hence, 
# only 40% of the isolates found in multi-strain clusters at TP2 were originally found in the TP1 dataset.
x <- 0.6
tp2_ms <- t2_height_select %>% set_colnames(c("isolate", "cluster")) %>% 
  right_join(., multi_strain, by = "cluster")
  
isolates_to_rm <- sample(tp2_ms$isolate, round(x*nrow(tp2_ms)))
tp1 <- tp2 %>% dplyr::filter(!(isolate %in% isolates_to_rm))
  
# TP2 clusters and sizes
t1_height_select <- tp1 %>% dplyr::select(isolate, all_of(h))
t1_sizes <- t1_height_select %>% pull(h) %>% table() %>% 
  as.data.frame() %>% as_tibble() %>% set_colnames(c("cluster", "size"))
t1_cl_over_one <- t1_sizes %>% filter(size > 1) %>% nrow()

num_ms <- list("ms_t1" = t1_cl_over_one, "ms_t2" = t2_cl_over_one)

# write.csv(tp1, "3692_2020-04-15_tree-clusters-dm/synthetic_tp1.csv", row.names = FALSE)
```

Then, using the same height for comparison, there are 215 multi-strain clusters at TP1.
So the number of multi-strain clusters grew by `r ((t2_cl_over_one - t1_cl_over_one)/t2_cl_over_one) %>% scales::percent()`.

## Cluster metric generation

```{r cluster_sizes_melt}
time1 <- "3692_2020-04-15_tree-clusters-dm/synthetic_tp1.csv" %>%
  read.csv(file = ., stringsAsFactors = FALSE, numerals = "no.loss") %>% as_tibble()
X <- 1

time2 <- "3692_2020-04-15_tree-clusters-dm/3692_2020-04-15_thresholds.csv" %>%
  read.csv(file = ., stringsAsFactors = FALSE, numerals = "no.loss") %>% as_tibble()
Y <- 2

# isolates at TP2 and not TP1
novel_isolates <- setdiff(time2$isolate, time1$isolate)
# isolates at TP1
isolates_t1 <- time1$isolate
# isolates at TP2
isolates_t2 <- time2$isolate
```

Dataframes of the clusters at each time point and their associated sizes (easier for data retrieval later on).

```{r}
sizes_for_tpX <- clusterSizes(time1, "1") %>% 
  set_colnames(c("TP1_h", "TP1_cl", "TP1_cl_size", "ID"))
sizes_for_tpY <- clusterSizes(time2, "2") %>% 
  set_colnames(c("TP2_h", "TP2_cl", "TP2_cl_size", "ID"))
```

Reorganizing the data so the time points can be merged simply

```{r}
# Melted time1 data: || isolate | height | cluster | id (height_cluster) ||
df1 <- melt(time1, id = "isolate") %>% as_tibble() %>% set_colnames(c("isolate", "height", "cluster"))
df1$height %<>% as.character()
df1$id <- paste0(df1$height, "-", df1$cluster)
df1_sizes <- df1 %>% set_colnames(c("Isolate", "TP1_h", "TP1_cl", "ID")) %>% 
  left_join(., sizes_for_tpX, by = c("TP1_h", "TP1_cl", "ID"))

# Melted time2 data: || isolate | height | cluster | id (height_cluster) ||
df2 <- melt(time2, id = "isolate") %>% as_tibble() %>% set_colnames(c("isolate", "height", "cluster"))
df2$height %<>% as.character()
df2$id <- paste0(df2$height, "-", df2$cluster)
df2_sizes <- df2 %>% set_colnames(c("Isolate", "TP2_h", "TP2_cl", "ID")) %>% 
  left_join(., sizes_for_tpY, by = c("TP2_h", "TP2_cl", "ID"))
```

The foundational frame of cluster data:
  - sizes and membership at each time point, 
  - as well as the proportional increase from TP1 to TP2, in decimal form
  
```{r}
df_all <- right_join(df1_sizes, df2_sizes, by = c("Isolate", "ID"))

all_clusters <- df_all %>% dplyr::select(Isolate, TP1_h, TP1_cl, TP2_h, TP2_cl, TP1_cl_size, TP2_cl_size)
all_clusters$TP1_cl_size[is.na(all_clusters$TP1_cl_size)] <- 0

all_clusters$prop_inc <- NA
all_clusters$prop_inc[all_clusters$TP1_cl_size == 0] <- 1
inds <- which(all_clusters$TP1_cl_size != 0)
all_clusters$prop_inc[inds] <- (all_clusters$TP2_cl_size[inds] - all_clusters$TP1_cl_size[inds]) / all_clusters$TP1_cl_size[inds]
```

## Adaptive threshold development

We can plot what a couple of different threshold models can look like. This part will be heavily altered depending on what the actual data looks like. Currently being used to model synthetic data, and what we might expect significant size change to look like. 

```{r}
x <- c(1, 25, 50, 100, 150)
y <- c(300, 150, 75, 30, 15)

# http://r-statistics.co/Loess-Regression-With-R.html
loessMod1 <- loess(y ~ x, span = 1)
smoothed1 <- predict(loessMod1)

loessMod5 <- loess(y ~ x, span = 5)
smoothed5 <- predict(loessMod5)

lm_df <- tibble(x, y, smoothed1, smoothed5) %>%
  set_colnames(c("x", "Preset values", "Local regression (span 1)", "Local regression (span 5)")) %>%
  melt(id = "x") %>% as_tibble() %>% set_colnames(c("Cluster size", "Function", "Growth"))

{ggplot(lm_df, aes(x = `Cluster size`, y = `Growth`, color = `Function`)) + geom_point() + 
    theme(legend.position = "bottom") + scale_y_continuous(labels = scales::percent) + 
    ylab("Growth (%)")} %>% ggplotly()
```

The model selection step, and using it to determine what the adaptive threshold function values would be for each input of initial cluster size. Anything that needs to be extrapolated is set to a pre-determined plateau value of percent increase. 

```{r}
model_used <- loessMod1
# predict.lm(model_used, data.frame(x = 30)) # note there are different types of prediction methods

predicted_y <- predict(model_used, newdata = all_clusters$TP1_cl_size)
na_predicted <- all_clusters$TP1_cl_size[which(is.na(predicted_y))]

if (all(na_predicted > max(lm_df$`Cluster size`))) {
  predicted_y[is.na(predicted_y)] <- 15
}

sizes_predicted <- predicted_y %>% round() %>% bind_cols(all_clusters, predicted = .)
sizes_predicted$predicted <- sizes_predicted$predicted*0.01
```

## Fold change
Ranking the data by the actual proportional change over the adaptive threshold requirement, to see by how much each cluster exceeds the growth prediction. The data is then saved to the current working directory. 

```{r}
sizes_predicted$fold_change <- sizes_predicted$prop_inc / sizes_predicted$predicted
sizes_predicted$fold_change %<>% round(., digits = 3)
sizes_predicted %<>% arrange(., -fold_change)

sizes_predicted$predicted %<>% scales::percent()
sizes_predicted$prop_inc %<>% scales::percent()

metrics <- sizes_predicted %>% 
  set_colnames(c("Isolate", "TP1 height", "TP1 cluster", "TP2 height", "TP2 cluster", 
                 "TP1 cluster size", "TP2 cluster size", "Proportional growth", 
                 "Adaptive threshold", "Fold change (Growth/Threshold)"))

# write.csv(metrics, "all_clusters_table.csv", row.names = FALSE)
# write.csv(metrics[1:10,], "cluster_metrics_first_ten.csv", row.names = FALSE)

stopwatch[2] <- Sys.time()
```

```{r}
paste0("It takes ", round((stopwatch[2]-stopwatch[1])/60, digits = 2), " minutes to run.") %>% print()
```

