---
title: "check_with_prev"
output: html_document
---

### SETUP:
Reads in the threshold data for time points 1 and 2 and creates two type of datasets: 
  (1) a "filtered" dataset, where the dataset only contains the "original" isolates (the ones found in the TP1 dataset), but with the cluster assignments from both time points
  (2) a "coded" dataset, where the entire list of isolates (original and novel) is replaced with numeric representations from the list of positive integers (this makes it easier to compare the composition of the clusters from TP1 to TP2, later on)
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("flagging_functions.R")

tp1_filename <- "data/timepoint1_data.csv" #input_args[1]      # "european-t1_clusters.csv"
tp2_filename <- "data/timepoint2_data.csv" #input_args[2]      # "european-t2_clusters.csv"

# the cluster assignments, in form: || isolates | height 0 | height 1 | ... ||
# the raw datasets, no filtering or other changes made
time1_raw <- readData(tp1_filename, X)
time2_raw <- readData(tp2_filename, Y)

# USER: make sure the first column is the isolate labeling
# then replace with "isolate" for easier manipulation later on
colnames(time1_raw)[1] <- colnames(time2_raw)[1] <- "isolate"

# the coded TP2 datasets (where the isolates are now replaced by positive integers)
time2_coded <- time2_raw
time2_coded$isolate <- 1:nrow(time2_coded)

# isolates found at TP1 (the original isolates):
isolates_tp1 <- time1_raw$isolate
 
# isolates found at TP2 (both original and novel isolates):
isolates_tp2 <- time2_raw$isolate

# isolates introduced at TP2 (novel isolates):
novels <- setdiff(isolates_tp2, isolates_tp1)

# the TP2 dataset filtered so it does not include the novels (only includes original isolates)
time2_filtered <- time2_filtered_coded <- time2_raw %>% dplyr::filter(!(isolate %in% novels))

# the filtered and then coded dataset (so novels are removed and then the originals get a code from the positive integers)
time2_filtered_coded$isolate <- 1:nrow(time2_filtered_coded)

# melted (long) format of the time point 1 dataset (note, contains assignments for all heights and isolates)
meltedTP1 <- time1_raw %>% 
  meltData(., "isolate") %>% 
  set_colnames(c("isolate", "tp1_h", "tp1_cl")) %>% 
  factorToInt(., "tp1_h")

ids <- list()
```

### BASE CASE:

```{r}
# this should be '0', the first column is the isolates
base_case_h <- colnames(time2_raw)[2]

# finding the composition (in terms of coded isolates) of each of the clusters
# note that this includes both novel and original isolates
precomp <- clustComp(time2_coded, base_case_h, "h_before")

# we find the cluster assignments for all isolates at a particular height at TP2 that can be 
# found in the given list of clusters (have not yet found originating clusters for these)
# dataset of form || isolate (originals) | tp2 height | tp2 cluster | tp2 cluster size ||
just_originals <- clusterAssignments(base_case_h, time2_raw, precomp$h_before, novels)

collectionMsg(base_case_h, time2_raw, precomp$h_before)
single_height <- resultsProcess(time2_raw, precomp$h_before, meltedTP1, ids, just_originals)
ids <- c(unlist(ids), single_height$flagged %>% unique()) %>% unique()
metrics <- addToMetrics(base_case_h, ids)

saveRDS(single_height, "height_data/h_0.Rds")
saveRDS(ids, "ids/h_0.Rds")
saveRDS(metrics, "num_ids/h_0.Rds")
```

```{r}
# height2 <- "944"
# single_height <- paste0("height_data/h_", height2, ".Rds") %>% readRDS()
# ids <- paste0("ids/h_", height2, ".Rds") %>% readRDS()
# metrics <- paste0("num_ids//h_", height2, ".Rds") %>% readRDS()
```


```{r}
stopwatch <- rep(0,3)
stopwatch[1] <- Sys.time()

# for (j in 1:50) {
for (j in 1:length(colnames(time2_raw)[-1][-1])) {
  stopwatch[2] <- Sys.time()
  # if (j == 900) {stop("start from here")}
  # HEIGHT 1 - b3 is the coded composition of all clusters at TP2 at this height 
  # which means time2_raw --> filtered --> coded
  height2 <- colnames(time2_raw)[-1][-1][j]
  message(paste0("Threshold h_", height2, " - ", j, " / ", length(colnames(time2_raw)[-1][-1])))
  
  # these are all the clusters and their composition
  postcomp <- clustComp(time2_coded, height2, "h_after")

  # these are the clusters that change in composition from h0 to h1
  # then for h1 at TP2, we only need to find the originating TP1 clusters for these ones, 
  # all other clusters have the same originating cluster as the TP2 h0 clusters
  
  # cluster composition that is found at the new height that were different before
  indices_new <- setdiff(postcomp$composition, precomp$composition)
  # going to analyze the clusters that are actually different at height2
  changed_comp <- postcomp %>% filter(composition %in% indices_new)  
  ac <- changed_comp$h_after
  
  # note that there are many clusters that exist at height1 but not at height2
  # (we are not interested in this representation)
  # the focus is on clusters that now exist at height2
  # clusters at height2 that are not in the set of those that changed in composition from height1
  stayed_the_same <- postcomp %>% 
    filter(!(composition %in% indices_new)) %>% 
    left_join(., precomp, by = "composition") %>% 
    select(-composition)

  # the result output for clusters that did not change in composition
  transit <- inner_join(single_height, stayed_the_same, by = c("tp2_cl" = "h_before")) %>% 
    select(-tp2_cl) %>% rename(tp2_cl = h_after) %>% select(colnames(single_height))
  transit$tp2_h <- height2
  
  precomp <- postcomp %>% set_colnames(c("composition", "h_before"))
  
  # if nrow(changed_comp) > 0, then there is >= one cluster different in composition from previous height
  if (nrow(changed_comp) > 0) {
    # "just_originals" only includes the original isolates (but still has the actual TP2 cluster sizes) and 
    # is a dataset of form || isolate | height (at tp2) | cluster (at tp2) | cluster size (at tp2) ||
    just_originals <- clusterAssignments(height2, time2_raw, ac, novels)
    collectionMsg(height2, time2_raw, precomp$h_before)
    new_height <- resultsProcess(time2_raw, ac, meltedTP1, ids, just_originals)
    ids <- c(ids, new_height$flagged) %>% unique()
    
    single_height <- new_height %>% bind_rows(transit, .)
    
  }else {
    message("No clusters changed from the previous height")
    single_height <- transit
  }
  
  metrics <- addToMetrics(height2, ids, metrics)
  
  saveData(height2, single_height, ids, metrics)
  
  message("\n")
  stopwatch[3] <- Sys.time()
}


# NOTE: ALSO NEED TO PREP A NICE WAY TO STOP IN THE MIDDLE OF RUNNING THIS AND PICK UP AGAIN LATER
# (LIKE A PARTICULAR WAY OF DOING SO CLEANLY, WITHOUT ANY WASTED WORK)
```


```{r}
hfiles <- list.files("height_data/")
tracked_clusters <- paste0("height_data/", hfiles[1]) %>% readRDS()

pb <- progress_bar$new(total = length(hfiles))
for (h_file in hfiles[-1]) {
  pb$tick()
  next_file <- paste0("height_data/", h_file) %>% readRDS()
  tracked_clusters <- bind_rows(tracked_clusters, next_file)
}
tracked_clusters <- tracked_clusters %>% 
  mutate(across(tp2_h, as.integer)) %>% 
  arrange(tp1_h, tp1_cl)

saveRDS(tracked_clusters, "results.Rds")
```

```{r}
# # PROBLEM
# # cluster 0-441 from TP1 is the originating cluster for
# #   - 945-686, 946-685, 947-684, ... need to check this
# #   - all the other rows should be flagged, since the originating cluster is the same
# tracked_clusters %>% 
#   filter(tp2_h %in% c(944,945)) %>% 
#   filter(tp1_cl == 444)
```


```{r}
# tracked_clusters is the list of original genomes and their cluster assignments at all heights as well 
# as their originating clusters, with clusters flagged if their originating cluster has been found before
# the clusters that actually changed from TP1 to TP2 are flagged with an NA
original_tracking <- readRDS("results.Rds") %>% createID(., "tp2_h", "tp2_cl")

# in this dataset we have, for all ORIGINAL isolates, the cluster assignments at TP1 and TP2, 
# the change in cluster size, and whether or not the originating cluster was found before

# the next step:
#   - for each novel isolate
#     - get the cluster assignments at all heights (for TP2)
#       - then match with the originating cluster at TP1 (if one existed)
#       - use the originating cluster set (so the flags are included)
#     - get the cluster sizes (TP1 and TP2)
# then:
#   - proportional increase, adaptive threshold, fold change
```

```{r}
# This part is just to check that all the TP2 clusters are represented
actual_clusters <- compClusters(time2_raw)

# definitely have originals and might have novels
omn <- time2_raw %>% 
  filter(!(isolate %in% novels)) %>% 
  compClusters(.)
omn_ids <- omn$id %>% unique() %>% sort()

# definitely have novels and might have originals
nmo <- time2_raw %>% 
  filter(isolate %in% novels) %>% 
  compClusters()
nmo_ids <- nmo$id %>% unique() %>% sort()

# novels only - present in the nmo set but not in the omn set
novels_only <- setdiff(nmo_ids, omn_ids)

# novels and originals - present in both the omn and nmo sets
present_in_both <- intersect(omn_ids, nmo_ids)

# originals only - present in the omn and not the nmo set
originals_only <- setdiff(omn_ids, nmo_ids)

in_tracked <- original_tracking %>% 
  select(tp2_h, tp2_cl, id) %>% 
  pull(id) %>% unique() %>% sort()

# checking that the actual clusters match with the output of the tracking process
sum(length(present_in_both), length(originals_only)) == length(in_tracked)

# # FOUND THE MISSING CLUSTERS! 
# #   - some of the clusters with clusters with originals are not being tracked (the results process missed them)
# 
# setA <- setdiff(present_in_both, in_tracked)
# length(setA)
# # 105 clusters that have both novels and originals are not being tracked
# setB <- setdiff(originals_only, in_tracked)
# length(setB)
# # 145 clusters that only have originals are not being tracked
# actual_clusters %>% filter(id %in% c(setA,setB)) %>% View()
# # all of these missing clusters are from height 1!
# #     --> but the results process is not actually skipping this height entirely
# #     --> some of the clusters are being mislabelled
# #     --> one of this missing clusters is 5 at height 1
```


```{r}
# get the TP2 cluster sizes
tmp <- time2_raw %>% 
  meltData(., "isolate") %>% 
  factorToInt("variable") %>% 
  set_colnames(c("isolate", "tp2_h", "tp2_cl")) %>% 
  createID(., "tp2_h", "tp2_cl") %>% 
  arrange(tp2_h, tp2_cl)

all_sizes <- tmp %>% group_by(tp2_h, tp2_cl) %>% 
  summarise(tp2_cl_size = n(), .groups = "drop") %>% 
  left_join(tmp, ., by = c("tp2_h", "tp2_cl"))

# novel cluster assignments
cwn <- time2_raw %>% 
  filter(isolate %in% novels) %>% 
  meltData(., "isolate") %>% 
  factorToInt("variable") %>% 
  set_colnames(c("isolate", "tp2_h", "tp2_cl")) %>% 
  createID(., "tp2_h", "tp2_cl")

# non-novel cluster assignments
cnn <- time2_raw %>% 
  filter(!(isolate %in% novels)) %>% 
  meltData(., "isolate") %>% 
  factorToInt("variable") %>% 
  set_colnames(c("isolate", "tp2_h", "tp2_cl")) %>% 
  createID(., "tp2_h", "tp2_cl")

# the original_tracking dataset has only original isolates
# we want to track the clusters for
#   - those with originals and novels
tmp1 <- original_tracking %>% select(-isolate) %>% unique()
oan <- all_sizes %>% filter(id %in% intersect(cwn$id, cnn$id)) %>% 
  left_join(., tmp1, by = c("tp2_h", "tp2_cl", "tp2_cl_size", "id"))
# identical(oan$id %>% unique() %>% sort(), a1 %>% sort() %>% unique()) # this should be true

#   - and create a new set with only the novels
nov_only <-  all_sizes %>% filter(id %in% setdiff(cwn$id, cnn$id)) %>% 
  add_column(tp1_h = NA, tp1_cl = NA, tp1_cl_size = 0, flagged = NA) %>% 
  select(isolate, tp1_h, tp1_cl, tp1_cl_size, tp2_h, tp2_cl, tp2_cl_size, flagged, id)


dfall <- original_tracking %>% 
  bind_rows(., nov_only) %>% 
  bind_rows(., oan)
```

```{r}
tracked <- dfall %>% add_column(prop_inc = NA)
# HANDLE THESE CASES SEPARATELY: 
zero_denoms <- tracked %>% filter(tp1_cl_size == 0)

# we now are only going to look at clusters that existed at TP1 first
tracked <- tracked %>% filter(tp1_cl_size > 0)

tracked$prop_inc <- (tracked$tp2_cl_size - tracked$tp1_cl_size) / tracked$tp1_cl_size
```

```{r}
## Adaptive threshold development

# We can plot what a couple of different threshold models can look like.
# This part will be heavily altered depending on what the actual data looks like.
# Currently being used to model synthetic data, and what we might expect significant
# size change to look like.

# message("Running local regression steps for adaptive threshold development")
x <- c(1, 25, 50, 100, 150)
y <- c(300, 150, 75, 30, 15)

# http://r-statistics.co/Loess-Regression-With-R.html
loessMod1 <- loess(y ~ x, span = 1)
smoothed1 <- predict(loessMod1)

loessMod5 <- loess(y ~ x, span = 5)
smoothed5 <- predict(loessMod5)

lm_df <- tibble(x, y, smoothed1, smoothed5) %>%
  set_colnames(c("x", "Preset values", "Local regression (span 1)", "Local regression (span 5)")) %>%
  meltData(., "x") %>% set_colnames(c("Cluster size", "Function", "Growth"))

# The model selection step, and using it to determine what the adaptive threshold
# function values would be for each input of initial cluster size. Anything that
# needs to be extrapolated is set to a pre-determined plateau value of percent increase.

model_used <- loessMod1
# predict.lm(model_used, data.frame(x = 30)) # note there are different types of prediction methods
predicted_y <- predict(model_used, newdata = tracked$tp1_cl_size)
na_predicted <- tracked$tp1_cl_size[which(is.na(predicted_y))]

if (all(na_predicted > max(lm_df$`Cluster size`))) {
  predicted_y[is.na(predicted_y)] <- 15
}

# ## Fold change
# # Ranking the data by the actual proportional change over the adaptive threshold requirement,
# # to see by how much each cluster exceeds the growth prediction. The data is then saved to
# # the current working directory.

final_df <- predicted_y %>% round() %>% bind_cols(tracked, predicted = .)
final_df$predicted <- final_df$predicted*0.01
final_df$fold_change <- (final_df$prop_inc / final_df$predicted) %>% 
  round(., digits = 3)
final_df <- final_df %>% arrange(., -fold_change) %>% 
  mutate(across(c(predicted, prop_inc), scales::percent))
```


```{r}
# at this point, the columns we have data for are:
#   - isolate
#   - tp1_h, tp1_cl, tp1_cl_size, 
#   - tp2_h, tp2_cl, tp2_cl_size, 
#   - flagged, id, prop_inc, predicted, fold_change
# We still need a column with the number of novels in each TP2 cluster (note that this may not correspond 
# exactly with the change in a cluster's size from TP1 to TP2)

nov_sizes <- time2_raw %>% 
  filter(isolate %in% novels) %>% 
  meltData(., "isolate") %>% 
  factorToInt("variable") %>% 
  set_colnames(c("isolate", "tp2_h", "tp2_cl")) %>% 
  createID(., "tp2_h", "tp2_cl") %>% 
  select(-isolate, -id) %>%
  arrange(tp2_h, tp2_cl) %>% 
  group_by_all() %>% 
  summarise(num_novels = n(), .groups = "drop")

# final_df --> nrow is 818 768
# zero_denoms --> 981 916

final_df <- final_df %>% left_join(., nov_sizes, by = c("tp2_h", "tp2_cl"))

zero_denoms$fold_change <- zero_denoms$predicted <- NA

final_df <- bind_rows(final_df, zero_denoms)
```

```{r}
# This part is just to check that all the TP2 clusters are represented (except for those with 
# only novels)
actual_clusters <- time2_raw %>% 
  meltData(., "isolate") %>% 
  select(-isolate) %>% unique() %>% 
  set_colnames(c("tp2_h", "tp2_cl")) %>% 
  factorToInt("tp2_h") %>% 
  arrange(tp2_h, tp2_cl) %>% 
  createID(., "tp2_h", "tp2_cl")
# 1 800 684

result_clusters <- final_df %>% 
  select(tp2_h, tp2_cl) %>% unique() %>% 
  arrange(tp2_h, tp2_cl) %>% 
  createID(., "tp2_h", "tp2_cl")

# if the following is true, then all clusters have been accounted for
# identical(actual_clusters$id, result_clusters$id)
```


```{r}
metrics <- final_df %>% 
  select(isolate, tp1_h, tp1_cl, tp2_h, tp2_cl, tp1_cl_size, tp2_cl_size, 
         flagged, num_novels, prop_inc, predicted, fold_change) %>% 
  set_colnames(c("Isolate", "Height (TP1)", "Cluster (TP1)", 
                 "Height (TP2)", "Cluster (TP2)", "TP1 cluster size", 
                 "TP2 cluster size", "Flag (originator seen before)", 
                 "Number of novels", "Proportional growth", 
                 "Adaptive threshold", "Fold change (Growth / Threshold)"))

write.csv(metrics, "all_clusters_table.csv", row.names = FALSE)
write.csv(metrics[1:10,], "first_ten_rows.csv", row.names = FALSE)
```

```{r}
metrics <- read.csv("all_clusters_table.csv", stringsAsFactors = FALSE) %>% as_tibble()

# problem:
#   - this dataset makes it look like every cluster absorbs at least one novel isolate
#   - what about the clusters that don't absorb any?
```

```{r}

```


```{r}
# metrics <- read.csv("first_ten_rows.csv", stringsAsFactors = FALSE, numerals = "no.loss") %>% as_tibble() %>% 
#   set_colnames(c("isolate", "tp1_h", "tp1_cl", "tp2_h", "tp2_cl", "tp1_cl_size", "tp2_cl_size", "flag", 
#                  "num_novels", "prop_inc", "adap_thresh", "fold_change"))
```




```{r}
# # Now need to make a dataframe that identifies the first multistrain cluster at TP2
# # that absorbs each of the novel genomes
# #   - then will use "tracked_clusters" to find the originating cluster at TP1
# #   - and can then output size changes for the novel genomes
# 
# 
# 
# # END GOAL: to have a list of the following:
# # || all isolates (novel and original) | tp1 height (all heights) | tp1 assigned cluster | 
# #   tp1 cluster size | tp2 height (all heights) | the cluster this isolate is found in at this tp2 height | 
# #   the size of this cluster | a flag for whether or not the tp1 cluster was the originating cluster |
# #   the change in cluster size | the proportional increase | ... ||
```

```{r}
# key_clusters <- metrics[which(is.na(metrics$`Flag (originator seen before)`)),] %>% 
#   select(-Isolate) %>% unique()
# # which(is.na(metrics$`Flag (originator seen before)`)) %>% length()
```

